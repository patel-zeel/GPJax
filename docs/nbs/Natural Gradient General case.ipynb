{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gpjax as gpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "from jax import jit, lax\n",
    "import optax as ox\n",
    "\n",
    "import gpjax as gpx\n",
    "from gpjax.natural_gradients import natural_gradients\n",
    "from gpjax.abstractions import progress_bar_scan\n",
    "\n",
    "#Set seed for reproducibility:\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(4)\n",
    "key = jr.PRNGKey(123)\n",
    "\n",
    "import typing as tp\n",
    "from copy import deepcopy\n",
    "\n",
    "import distrax as dx\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "from jax import lax, value_and_grad, jacobian\n",
    "from jaxtyping import f64\n",
    "\n",
    "from gpjax.config import get_defaults\n",
    "from gpjax.gps import AbstractPosterior\n",
    "from gpjax.parameters import (\n",
    "    build_identity,\n",
    "    build_trainables_false,\n",
    "    build_trainables_true,\n",
    "    trainable_params,\n",
    "    transform,\n",
    ")\n",
    "from gpjax.types import Dataset\n",
    "from gpjax.utils import I\n",
    "from gpjax.variational_families import (\n",
    "    AbstractVariationalFamily,\n",
    "    ExpectationVariationalGaussian,\n",
    "    NaturalVariationalGaussian,\n",
    ")\n",
    "from gpjax.variational_inference import StochasticVI\n",
    "DEFAULT_JITTER = get_defaults()[\"jitter\"]\n",
    "\n",
    "\n",
    "from gpjax.natural_gradients import natural_gradients, natural_to_expectation, _expectation_elbo, _stop_gradients_nonmoments, _stop_gradients_moments, fit_natgrads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and inducing points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5000\n",
    "noise = 0.2\n",
    "\n",
    "x = jr.uniform(key=key, minval=-5.0, maxval=5.0, shape=(n,)).sort().reshape(-1, 1)\n",
    "f = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\n",
    "signal = f(x)\n",
    "y = signal + jr.normal(key, shape=signal.shape) * noise\n",
    "\n",
    "D = gpx.Dataset(X=x, y=y)\n",
    "Dbatched = D.cache().repeat().shuffle(D.n).batch(batch_size=256).prefetch(buffer_size=1)\n",
    "\n",
    "xtest = jnp.linspace(-5.5, 5.5, 500).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = jnp.linspace(-5.0, 5.0, 2).reshape(-1, 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(x, y, \"o\", alpha=0.3)\n",
    "ax.plot(xtest, f(xtest))\n",
    "[ax.axvline(x=z_i, color=\"black\", alpha=0.3, linewidth=1) for z_i in z]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natgrads code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def natural_gradients(\n",
    "    stochastic_vi: StochasticVI,\n",
    "    train_data: Dataset,\n",
    "    transformations: dict,\n",
    "    xi_to_nat: tp.Callable[[tp.Dict], tp.Dict],\n",
    "    nat_to_xi: tp.Callable[[tp.Dict], tp.Dict],\n",
    ") -> tp.Tuple[tp.Callable[[dict, Dataset], dict]]:\n",
    "    \"\"\"\n",
    "    Computes natural gradients for variational Gaussian.\n",
    "    Args:\n",
    "        posterior: An instance of AbstractPosterior.\n",
    "        variational_family: An instance of AbstractVariationalFamily.\n",
    "        train_data: A Dataset.\n",
    "        transformations: A dictionary of transformations.\n",
    "    Returns:\n",
    "        Tuple[tp.Callable[[dict, Dataset], dict]]: Functions that compute natural gradients and hyperparameter gradients respectively.\n",
    "    \"\"\"\n",
    "    posterior = stochastic_vi.posterior\n",
    "    variational_family = stochastic_vi.variational_family\n",
    "\n",
    "    # The ELBO under the user chosen parameterisation xi.\n",
    "    xi_elbo = stochastic_vi.elbo(train_data, transformations, negative=True)\n",
    "\n",
    "    # The ELBO under the expectation parameterisation, L(η).\n",
    "    expectation_elbo = _expectation_elbo(posterior, variational_family, train_data)\n",
    "\n",
    "    if isinstance(variational_family, NaturalVariationalGaussian):\n",
    "\n",
    "        def nat_grads_fn(params: dict, trainables: dict, batch: Dataset) -> dict:\n",
    "            \"\"\"\n",
    "            Computes the natural gradients of the ELBO.\n",
    "            Args:\n",
    "                params: A dictionary of parameters.\n",
    "                trainables: A dictionary of trainables.\n",
    "                batch: A Dataset.\n",
    "            Returns:\n",
    "                dict: A dictionary of natural gradients.\n",
    "            \"\"\"\n",
    "            # Transform parameters to constrained space.\n",
    "            params = transform(params, transformations)\n",
    "\n",
    "            # Get natural moments θ.\n",
    "            natural_moments = params[\"variational_family\"][\"moments\"]\n",
    "\n",
    "            # Get expectation moments η.\n",
    "            expectation_moments = natural_to_expectation(natural_moments)\n",
    "\n",
    "            # Full params with expectation moments.\n",
    "            expectation_params = deepcopy(params)\n",
    "            expectation_params[\"variational_family\"][\"moments\"] = expectation_moments\n",
    "\n",
    "            # Compute gradient ∂L/∂η:\n",
    "            def loss_fn(params: dict, batch: Dataset) -> f64[\"1\"]:\n",
    "                # Determine hyperparameters that should be trained.\n",
    "                trains = deepcopy(trainables)\n",
    "                trains[\"variational_family\"][\"moments\"] = build_trainables_true(\n",
    "                    params[\"variational_family\"][\"moments\"]\n",
    "                )\n",
    "                params = trainable_params(params, trains)\n",
    "\n",
    "                # Stop gradients for non-moment parameters.\n",
    "                params = _stop_gradients_nonmoments(params)\n",
    "\n",
    "                return expectation_elbo(params, batch)\n",
    "\n",
    "            value, dL_dnat = value_and_grad(loss_fn)(expectation_params, batch)\n",
    "\n",
    "            # This is a renaming of the gradient components to match the natural parameterisation pytree.\n",
    "            natural_gradient = dL_dnat\n",
    "            natural_gradient[\"variational_family\"][\"moments\"] = {\n",
    "                \"natural_vector\": dL_dnat[\"variational_family\"][\"moments\"][\n",
    "                    \"expectation_vector\"\n",
    "                ],\n",
    "                \"natural_matrix\": dL_dnat[\"variational_family\"][\"moments\"][\n",
    "                    \"expectation_matrix\"\n",
    "                ],\n",
    "            }\n",
    "\n",
    "            return value, natural_gradient\n",
    "\n",
    "    else:\n",
    "\n",
    "        def nat_grads_fn(params: dict, trainables: dict, batch: Dataset) -> dict:\n",
    "            # Transform parameters to constrained space.\n",
    "            params = transform(params, transformations)\n",
    "\n",
    "            # Get natural moments θ.\n",
    "            natural_moments = xi_to_nat(params[\"variational_family\"][\"moments\"])\n",
    "\n",
    "            # Get expectation moments η.\n",
    "            expectation_moments = natural_to_expectation(natural_moments)\n",
    "\n",
    "            # Gradient function ∂ξ/∂θ:\n",
    "            dxi_dnat = jacobian(nat_to_xi)(natural_moments)\n",
    "\n",
    "           # Full params with expectation moments.\n",
    "            expectation_params = deepcopy(params)\n",
    "            expectation_params[\"variational_family\"][\"moments\"] = expectation_moments\n",
    "\n",
    "            # Compute gradient ∂L/∂η:\n",
    "            def loss_fn(params: dict, batch: Dataset) -> f64[\"1\"]:\n",
    "                # Determine hyperparameters that should be trained.\n",
    "                trains = deepcopy(trainables)\n",
    "                trains[\"variational_family\"][\"moments\"] = build_trainables_true(\n",
    "                    params[\"variational_family\"][\"moments\"]\n",
    "                )\n",
    "                params = trainable_params(params, trains)\n",
    "\n",
    "                # Stop gradients for non-moment parameters.\n",
    "                params = _stop_gradients_nonmoments(params)\n",
    "\n",
    "                return expectation_elbo(params, batch)\n",
    "\n",
    "            value, dL_dexp = value_and_grad(loss_fn)(expectation_params, batch)\n",
    "\n",
    "            \n",
    "            # The issue is combining: ∂ξ/∂θ ∂L/∂η\n",
    "            natural_gradient = None\n",
    "            \n",
    "            return value, natural_gradient\n",
    "\n",
    "    def hyper_grads_fn(params: dict, trainables: dict, batch: Dataset) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the hyperparameter gradients of the ELBO.\n",
    "        Args:\n",
    "            params: A dictionary of parameters.\n",
    "            trainables: A dictionary of trainables.\n",
    "            batch: A Dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary of hyperparameter gradients.\n",
    "        \"\"\"\n",
    "\n",
    "        def loss_fn(params: dict, batch: Dataset) -> f64[\"1\"]:\n",
    "            # Determine hyperparameters that should be trained.\n",
    "            params = trainable_params(params, trainables)\n",
    "\n",
    "            # Stop gradients for the moment parameters.\n",
    "            params = _stop_gradients_moments(params)\n",
    "\n",
    "            return xi_elbo(params, batch)\n",
    "\n",
    "        value, dL_dhyper = value_and_grad(loss_fn)(params, batch)\n",
    "\n",
    "        return value, dL_dhyper\n",
    "\n",
    "    return nat_grads_fn, hyper_grads_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider using the expectation family as a test for computing natural gradients $\\xi = \\eta$ (though of course this simplifies in reality: $\\frac{d\\xi}{d\\theta} \\frac{d\\mathcal{L}}{d\\eta} = \\frac{d\\mathcal{L}}{d\\theta}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by defining the bijection between $\\xi$ and $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xi_to_nat(moments: dict) -> dict:\n",
    "    \n",
    "    expectation_vector = moments[\"expectation_vector\"]\n",
    "    expectation_matrix = moments[\"expectation_matrix\"]\n",
    "    \n",
    "    m = expectation_vector.shape[0]\n",
    "    \n",
    "    mu = expectation_vector\n",
    "    \n",
    "    S = expectation_matrix - jnp.matmul(mu, mu.T)\n",
    "    S += I(m) * 1e-6\n",
    "    \n",
    "    L = jnp.linalg.cholesky(S)\n",
    "    \n",
    "    L_inv = jsp.linalg.solve_triangular(L, S, lower=True)\n",
    "    \n",
    "    S_inv = jnp.matmul(L_inv.T, L_inv)\n",
    "    \n",
    "    natural_matrix = - 0.5 * S_inv\n",
    "    natural_vector = jnp.matmul(S_inv, mu)\n",
    "    \n",
    "    return {\"natural_matrix\": natural_matrix, \"natural_vector\": natural_vector}\n",
    "\n",
    "def nat_to_xi(moments: dict) -> dict:\n",
    "    \n",
    "    natural_vector = moments[\"natural_vector\"]\n",
    "    natural_matrix = moments[\"natural_matrix\"]\n",
    "    \n",
    "    m = natural_vector.shape[0]\n",
    "    \n",
    "    S_inv = -2 * natural_matrix\n",
    "    S_inv += I(m) * 1e-6\n",
    "    L = jnp.linalg.cholesky(S_inv)\n",
    "    \n",
    "    C = jsp.linalg.solve_triangular(L, I(m), lower=True)\n",
    "    S = jnp.matmul(C.T, C)\n",
    "    \n",
    "    mu = jnp.matmul(S, natural_vector)\n",
    "    \n",
    "    expectation_vector = mu\n",
    "    expectation_matrix = S + jnp.matmul(mu, mu.T)\n",
    "    \n",
    "    \n",
    "    return {\"expectation_vector\": expectation_vector, \"expectation_matrix\": expectation_matrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpx.Gaussian(num_datapoints=n)\n",
    "kernel = gpx.RBF()\n",
    "prior = gpx.Prior(kernel=kernel)\n",
    "p =  prior * likelihood\n",
    "\n",
    "\n",
    "q = gpx.NaturalVariationalGaussian(prior=prior, inducing_inputs=z)\n",
    "\n",
    "q = gpx.ExpectationVariationalGaussian(prior=prior, inducing_inputs=z)\n",
    "\n",
    "svgp = gpx.StochasticVI(posterior=p, variational_family=q)\n",
    "\n",
    "\n",
    "params, trainables, constrainers, unconstrainers = gpx.initialise(svgp)\n",
    "\n",
    "params = gpx.transform(params, unconstrainers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then obtain our gradient functions as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nat_grads_fn, hyper_grads_fn = natural_gradients(svgp, D, constrainers, xi_to_nat = xi_to_nat, nat_to_xi = nat_to_xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And evaluate them e.g., like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nat_grads_fn(params=params, trainables=trainables, batch=D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a tuple of the loss function value and the gradient that is None for now, as we have not implemented it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, we won't see these as we have a training loop abstraction that could look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_params = fit_natgrads(svgp,\n",
    "                                   params = params,\n",
    "                                   trainables = trainables,   \n",
    "                                   transformations = constrainers,\n",
    "                                   train_data = Dbatched,\n",
    "                                   n_iters = 5000,\n",
    "                                   xi_to_nat= xi_to_nat,\n",
    "                                   nat_to_xi = nat_to_xi\n",
    ")\n",
    "\n",
    "learned_params = gpx.transform(learned_params, constrainers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7eb1cfec58eecaa2e5422163254bd25a3275ed109df9a51c3c95d775723db6f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
