{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67bee04b",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "In this notebook we demonstrate how to perform inference for Gaussian process models with non-Gaussian likelihoods via maximum a posteriori (MAP) and Markov chain Monte Carlo (MCMC). We focus on a classification task here and use [BlackJax](https://github.com/blackjax-devs/blackjax/) for sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9abeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "import blackjax\n",
    "import gpjax as gpx\n",
    "import optax as ox\n",
    "import distrax as dx\n",
    "from gpjax.utils import I\n",
    "import jax.scipy as jsp\n",
    "from jaxtyping import f64\n",
    "\n",
    "key = jr.PRNGKey(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ffe600",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "With the necessary modules imported, we simulate a dataset $\\mathcal{D} = (, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{100}$ with inputs $\\boldsymbol{x}$ sampled uniformly on $(-1., 1)$ and corresponding binary outputs\n",
    "\n",
    "$$\\boldsymbol{y} = 0.5 * \\text{sign}(\\cos(2 *  + \\boldsymbol{\\epsilon})) + 0.5, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N} \\left(\\textbf{0}, \\textbf{I} * (0.05)^{2} \\right).$$\n",
    "\n",
    "We store our data $\\mathcal{D}$ as a GPJax `Dataset` and create test inputs for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb63c4f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "x = jnp.sort(jr.uniform(key, shape=(100, 1), minval=-1.0, maxval=1.0), axis=0)\n",
    "y = 0.5 * jnp.sign(jnp.cos(3 * x + jr.normal(key, shape=x.shape) * 0.05)) + 0.5\n",
    "\n",
    "D = gpx.Dataset(X=x, y=y)\n",
    "\n",
    "xtest = jnp.linspace(-1.0, 1.0, 500).reshape(-1, 1)\n",
    "plt.plot(x, y, \"o\", markersize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b37730",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## MAP inference\n",
    "\n",
    "We begin by defining a Gaussian process prior with a radial basis function (RBF) kernel, chosen for the purpose of exposition. Since our observations are binary, we choose a Bernoulli likelihood with a probit link function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c21d6b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "kernel = gpx.RBF()\n",
    "prior = gpx.Prior(kernel=kernel)\n",
    "likelihood = gpx.Bernoulli(num_datapoints=D.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e600e6d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We construct the posterior through the product of our prior and likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbd9eed",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "posterior = prior * likelihood\n",
    "print(type(posterior))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec5011",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Whilst the latent function is Gaussian, the posterior distribution is non-Gaussian since our generative model first samples the latent GP and propagates these samples through the likelihood function's inverse link function. This step prevents us from being able to analytically integrate the latent function's values out of our posterior, and we must instead adopt alternative inference techniques. We begin with maximum a posteriori (MAP) estimation, a fast inference procedure to obtain point estimates for the latent function and the kernel's hyperparameters by maximising the marginal log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c61d2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "To begin we obtain an initial parameter state through the `initialise` callable (see the [regression notebook](https://gpjax.readthedocs.io/en/latest/nbs/regression.html)). We can obtain a MAP estimate by optimising the marginal log-likelihood with Optax's optimisers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06db8fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_state = gpx.initialise(posterior)\n",
    "negative_mll = jax.jit(posterior.marginal_log_likelihood(D, negative=True))\n",
    "\n",
    "optimiser = ox.adam(learning_rate=0.01)\n",
    "\n",
    "inference_state = gpx.fit(\n",
    "    objective=negative_mll,\n",
    "    parameter_state=parameter_state,\n",
    "    optax_optim=optimiser,\n",
    "    n_iters=1000,\n",
    ")\n",
    "\n",
    "map_estimate , training_history = inference_state.unpack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35341fc2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "From which we can make predictions at novel inputs, as illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd63d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dist = posterior(D, map_estimate)(xtest)\n",
    "\n",
    "predictive_dist = likelihood(latent_dist, map_estimate)\n",
    "\n",
    "predictive_mean = predictive_dist.mean()\n",
    "predictive_std = predictive_dist.stddev()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(x, y, \"o\", label=\"Observations\", color=\"tab:red\")\n",
    "ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=\"tab:blue\")\n",
    "ax.fill_between(\n",
    "    xtest.squeeze(),\n",
    "    predictive_mean - predictive_std,\n",
    "    predictive_mean + predictive_std,\n",
    "    alpha=0.2,\n",
    "    color=\"tab:blue\",\n",
    "    label='Two sigma',\n",
    ")\n",
    "ax.plot(xtest, predictive_mean - predictive_std, color=\"tab:blue\", linestyle=\"--\", linewidth=1)\n",
    "ax.plot(xtest, predictive_mean + predictive_std, color=\"tab:blue\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d35b2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "However, as a point estimate, MAP estimation is severely limited for uncertainty quantification, providing only a single piece of information about the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb257cd3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Laplace approximation\n",
    "The Laplace approximation improves uncertainty quantification by incorporating curvature induced by the marginal log-likelihood's Hessian to construct an approximate Gaussian distribution centered on the MAP estimate. Writing $\\tilde{p}(\\boldsymbol{f}|\\mathcal{D}) = p(\\boldsymbol{y}|\\boldsymbol{f}) p(\\boldsymbol{f})$ as the unormalised posterior for function values $\\boldsymbol{f}$ at the datapoints $\\boldsymbol{x}$, we can expand the log of this about the posterior mode $\\hat{\\boldsymbol{f}}$ via a Taylor expansion. This gives:\n",
    "\n",
    "\\begin{align}\n",
    "\\log\\tilde{p}(\\boldsymbol{f}|\\mathcal{D}) = \\log\\tilde{p}(\\hat{\\boldsymbol{f}}|\\mathcal{D}) + \\left[\\nabla \\log\\tilde{p}({\\boldsymbol{f}}|\\mathcal{D})|_{\\hat{\\boldsymbol{f}}}\\right]^{T} (\\boldsymbol{f}-\\hat{\\boldsymbol{f}}) + \\frac{1}{2} (\\boldsymbol{f}-\\hat{\\boldsymbol{f}})^{T} \\left[\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} \\right] (\\boldsymbol{f}-\\hat{\\boldsymbol{f}}) + \\mathcal{O}(\\lVert \\boldsymbol{f} - \\hat{\\boldsymbol{f}} \\rVert^3).\n",
    "\\end{align}\n",
    "\n",
    "Now since $\\nabla \\log\\tilde{p}({\\boldsymbol{f}}|\\mathcal{D})$ is zero at the mode, this suggests the following approximation\n",
    "\\begin{align}\n",
    "\\tilde{p}(\\boldsymbol{f}|\\mathcal{D}) \\approx \\log\\tilde{p}(\\hat{\\boldsymbol{f}}|\\mathcal{D}) \\exp\\left\\{ \\frac{1}{2} (\\boldsymbol{f}-\\hat{\\boldsymbol{f}})^{T} \\left[-\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} \\right] (\\boldsymbol{f}-\\hat{\\boldsymbol{f}}) \\right\\}\n",
    "\\end{align},\n",
    "\n",
    "that we identify as a Gaussian distribution,  $p(\\boldsymbol{f}| \\mathcal{D}) \\approx \\mathcal{N}(\\hat{\\boldsymbol{f}}, [-\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} ]^{-1} )$. Since the negative Hessian is positive definite, we can use the Cholesky decomposition to obtain the covariance matrix of the Laplace approximation at the datapoints below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a57ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_map_estimate = posterior(D, map_estimate)(x).mean()\n",
    "\n",
    "# Negative Hessian:\n",
    "H = jax.jacfwd(jax.jacrev(negative_mll))(map_estimate)[\"latent\"][\"latent\"][:,0,:,0]\n",
    "\n",
    "# LLᵀ = H\n",
    "jitter = 1e-6\n",
    "L = jnp.linalg.cholesky(H + I(D.n) * jitter)\n",
    "\n",
    "# H⁻¹ = H⁻¹ I = (LLᵀ)⁻¹ I = L⁻ᵀL⁻¹ I\n",
    "L_inv = jsp.linalg.solve_triangular(L, I(D.n), lower=True)\n",
    "H_inv = jsp.linalg.solve_triangular(L.T, L_inv, lower=False)\n",
    "\n",
    "laplace_approximation = dx.MultivariateNormalFullCovariance(f_map_estimate, H_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0cf349",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "For novel inputs, we must project the above distribution, which can be achived via the function defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6cb53e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from gpjax.types import  Dataset\n",
    "from gpjax.kernels import gram, cross_covariance\n",
    "\n",
    "\n",
    "def predict(map_estimate: dict, laplace_at_data: dx.Distribution, train_data: Dataset, test_inputs: f64[\"N D\"], jitter: int = 1e-6) ->  dx.Distribution:\n",
    "    \"\"\"Compute the predictive distribution of the Laplace approximation at novel inputs.\n",
    "\n",
    "    Args:\n",
    "        laplace_at_data (dict): The Laplace approximation at the datapoints.\n",
    "\n",
    "    Returns:\n",
    "        dx.Distribution: The Laplace approximation at novel inputs.\n",
    "    \"\"\"\n",
    "    x, n = train_data.X, train_data.n\n",
    "\n",
    "    t = test_inputs\n",
    "    n_test = t.shape[0]\n",
    "\n",
    "    mu = laplace_at_data.mean().reshape(-1, 1)\n",
    "    cov = laplace_at_data.covariance()\n",
    "\n",
    "    Ktt = gram(prior.kernel, t, map_estimate[\"kernel\"])\n",
    "    Kxx = gram(prior.kernel, x, map_estimate[\"kernel\"])\n",
    "    Kxt = cross_covariance(prior.kernel, x, t, map_estimate[\"kernel\"])\n",
    "    μt = prior.mean_function(t, map_estimate[\"mean_function\"])\n",
    "    μx = prior.mean_function(x, map_estimate[\"mean_function\"])\n",
    "\n",
    "    # Lx Lxᵀ = Kxx\n",
    "    Lx = jnp.linalg.cholesky(Kxx + I(n) * jitter)\n",
    "\n",
    "    # sqrt sqrtᵀ = Σ\n",
    "    sqrt = jnp.linalg.cholesky(cov + I(n) * jitter)\n",
    "\n",
    "    # Lz⁻¹ Kxt\n",
    "    Lx_inv_Kxt = jsp.linalg.solve_triangular(Lx, Kxt, lower=True)\n",
    "\n",
    "    # Kxx⁻¹ Kxt\n",
    "    Kxx_inv_Kxt = jsp.linalg.solve_triangular(Lx.T, Lx_inv_Kxt, lower=False)\n",
    "\n",
    "    # Ktx Kxx⁻¹ sqrt\n",
    "    Ktx_Kxx_inv_sqrt = jnp.matmul(Kxx_inv_Kxt.T, sqrt)\n",
    "    \n",
    "    # μt + Ktx Kxx⁻¹ (μ - μx)\n",
    "    mean = μt + jnp.matmul(Kxx_inv_Kxt.T, mu - μx)\n",
    "\n",
    "    # Ktt  -  Ktx Kxx⁻¹ Kxt  +  Ktx Kxx⁻¹ S Kxx⁻¹ Kxt\n",
    "    covariance = Ktt - jnp.matmul(Lx_inv_Kxt.T, Lx_inv_Kxt) + jnp.matmul(Ktx_Kxx_inv_sqrt, Ktx_Kxx_inv_sqrt.T)\n",
    "    covariance += I(n_test) * jitter\n",
    "\n",
    "    return dx.MultivariateNormalFullCovariance(\n",
    "        jnp.atleast_1d(mean.squeeze()), covariance\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd8398",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "From this we can construct the predictive distribution at the test points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1f8e12",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "latent_dist = predict(map_estimate, laplace_approximation, D, xtest)\n",
    "\n",
    "predictive_dist = likelihood(latent_dist, map_estimate)\n",
    "\n",
    "predictive_mean = predictive_dist.mean()\n",
    "predictive_std = predictive_dist.stddev()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(x, y, \"o\", label=\"Observations\", color=\"tab:red\")\n",
    "ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=\"tab:blue\")\n",
    "ax.fill_between(\n",
    "    xtest.squeeze(),\n",
    "    predictive_mean - predictive_std,\n",
    "    predictive_mean + predictive_std,\n",
    "    alpha=0.2,\n",
    "    color=\"tab:blue\",\n",
    "    label='Two sigma',\n",
    ")\n",
    "ax.plot(xtest, predictive_mean - predictive_std, color=\"tab:blue\", linestyle=\"--\", linewidth=1)\n",
    "ax.plot(xtest, predictive_mean + predictive_std, color=\"tab:blue\", linestyle=\"--\", linewidth=1)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243115de",
   "metadata": {},
   "source": [
    "However, the Laplace approximation is still limited by considering information about the posterior at a single location. On the other hand, through approximate sampling, MCMC methods allow us to learn all information about the posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5722234",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## MCMC inference\n",
    "\n",
    "At the high level, an MCMC sampler works by starting at an initial position and drawing a sample from a cheap-to-simulate distribution known as the _proposal_. The next step is to determine whether this sample could be considered a draw from the posterior. We accomplish this using an _acceptance probability_ determined via the sampler's _transition kernel_ which depends on the current position and the unnormalised target posterior distribution. If the new sample is more _likely_, we accept it; otherwise, we reject it and stay in our current position. Repeating these steps results in a Markov chain (a random sequence that depends only on the last state) whose stationary distribution (the long-run empirical distribution of the states visited) is the posterior. For a gentle introduction, see the first chapter of [A Handbook of Markov Chain Monte Carlo](https://www.mcmchandbook.net/HandbookChapter1.pdf).\n",
    "\n",
    "### MCMC through BlackJax\n",
    "\n",
    "Rather than implementing a suite of MCMC samplers, GPJax relies on MCMC-specific libraries for sampling functionality. We focus on [BlackJax](https://github.com/blackjax-devs/blackjax/) in this notebook, which we recommend adopting for general applications. However, we also support TensorFlow Probability as demonstrated in the [TensorFlow Probability Integration notebook](https://gpjax.readthedocs.io/en/latest/nbs/tfp_integration.html).\n",
    "\n",
    "We'll use the No U-Turn Sampler (NUTS) implementation given in BlackJax for sampling. For the interested reader, NUTS is a Hamiltonian Monte Carlo sampling scheme where the number of leapfrog integration steps is computed at each step of the change according to the NUTS algorithm. In general, samplers constructed under this framework are very efficient.\n",
    "\n",
    "We begin by generating _sensible_ initial positions for our sampler before defining an inference loop and sampling 500 values from our Markov chain. In practice, drawing more samples will be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from BlackJax's introduction notebook.\n",
    "num_adapt = 500\n",
    "num_samples = 500\n",
    "\n",
    "params, trainables, bijectors = gpx.initialise(posterior, key).unpack()\n",
    "mll = posterior.marginal_log_likelihood(D, negative=False)\n",
    "unconstrained_mll = jax.jit(lambda params: mll(gpx.constrain(params, bijectors)))\n",
    "\n",
    "adapt = blackjax.window_adaptation(blackjax.nuts, mll, num_adapt, target_acceptance_rate=0.65)\n",
    "\n",
    "# Initialise the chain\n",
    "unconstrained_params = gpx.unconstrain(params, bijectors)\n",
    "last_state, kernel, _ = adapt.run(key, unconstrained_params)\n",
    "\n",
    "def inference_loop(rng_key, kernel, initial_state, num_samples):\n",
    "    def one_step(state, rng_key):\n",
    "        state, info = kernel(rng_key, state)\n",
    "        return state, (state, info)\n",
    "\n",
    "    keys = jax.random.split(rng_key, num_samples)\n",
    "    _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)\n",
    "\n",
    "    return states, infos\n",
    "\n",
    "\n",
    "# Sample from the posterior distribution\n",
    "states, infos = inference_loop(key, kernel, last_state, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda1c0b0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Sampler efficiency\n",
    "\n",
    "BlackJax gives us easy access to our sampler's efficiency through metrics such as the sampler's _acceptance probability_ (the number of times that our chain accepted a proposed sample, divided by the total number of steps run by the chain). For NUTS and Hamiltonian Monte Carlo sampling, we typically seek an acceptance rate of 60-70% to strike the right balance between having a chain which is _stuck_ and rarely moves versus a chain that is too jumpy with frequent small steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc9849",
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptance_rate = jnp.mean(infos.acceptance_probability)\n",
    "print(f\"Acceptance rate: {acceptance_rate:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbcdcc1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Our acceptance rate is slightly too large, prompting an examination of the chain's trace plots. A well-mixing chain will have very few (if any) flat spots in its trace plot whilst also not having too many steps in the same direction. In addition to the model's hyperparameters, there will be 500 samples for each of the 100 latent function values in the `states.position` dictionary. We depict the chains that correspond to the model hyperparameters and the first value of the latent function for brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe9f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(15, 5), tight_layout=True)\n",
    "ax0.plot(states.position[\"kernel\"][\"lengthscale\"])\n",
    "ax1.plot(states.position[\"kernel\"][\"variance\"])\n",
    "ax2.plot(states.position[\"latent\"][:, 0, :])\n",
    "ax0.set_title(\"Kernel Lengthscale\")\n",
    "ax1.set_title(\"Kernel Variance\")\n",
    "ax2.set_title(\"Latent Function (index = 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbac34fa",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Prediction\n",
    "\n",
    "Having obtained samples from the posterior, we draw ten instances from our model's predictive distribution per MCMC sample. Using these draws, we will be able to compute credible values and expected values under our posterior distribution.\n",
    "\n",
    "An ideal Markov chain would have samples completely uncorrelated with their neighbours after a single lag. However, in practice, correlations often exist within our chain's sample set. A commonly used technique to try and reduce this correlation is _thinning_ whereby we select every $n$th sample where $n$ is the minimum lag length at which we believe the samples are uncorrelated. Although further analysis of the chain's autocorrelation is required to find appropriate thinning factors, we employ a thin factor of 10 for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1006535",
   "metadata": {},
   "outputs": [],
   "source": [
    "thin_factor = 10\n",
    "samples = []\n",
    "\n",
    "for i in range(0, num_samples, thin_factor):\n",
    "    ps = gpx.parameters.copy_dict_structure(params)\n",
    "    ps[\"kernel\"][\"lengthscale\"] = states.position[\"kernel\"][\"lengthscale\"][i]\n",
    "    ps[\"kernel\"][\"variance\"] = states.position[\"kernel\"][\"variance\"][i]\n",
    "    ps[\"latent\"] = states.position[\"latent\"][i, :, :]\n",
    "    ps = gpx.constrain(ps, bijectors)\n",
    "\n",
    "    latent_dist = posterior(D, ps)(xtest)\n",
    "    predictive_dist = likelihood(latent_dist, ps)\n",
    "    samples.append(predictive_dist.sample(seed=key, sample_shape=(10,)))\n",
    "\n",
    "samples = jnp.vstack(samples)\n",
    "\n",
    "lower_ci, upper_ci = jnp.percentile(samples, jnp.array([2.5, 97.5]), axis=0)\n",
    "expected_val = jnp.mean(samples, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a36562",
   "metadata": {},
   "source": [
    "\n",
    "Finally, we end this tutorial by plotting the predictions obtained from our model against the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb82c076",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 5), tight_layout=True)\n",
    "ax.plot(x, y, \"o\", markersize=5, color=\"tab:red\", label=\"Observations\", zorder=2, alpha=0.7)\n",
    "ax.plot(xtest, expected_val, linewidth=2, color=\"tab:blue\", label=\"Predicted mean\", zorder=1)\n",
    "ax.fill_between(\n",
    "    xtest.flatten(),\n",
    "    lower_ci.flatten(),\n",
    "    upper_ci.flatten(),\n",
    "    alpha=0.2,\n",
    "    color=\"tab:blue\",\n",
    "    label=\"95% CI\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177fec8f",
   "metadata": {},
   "source": [
    "## System configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd4c5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w -a \"Thomas Pinder & Daniel Dodd\""
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "custom_cell_magics": "kql"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
